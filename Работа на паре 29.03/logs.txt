
==> Audit <==
|------------|--------------------------------|----------|------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  | User | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|------|---------|---------------------|---------------------|
| start      | --memory=2048mb                | minikube | mgpu | v1.35.0 | 29 Mar 25 15:13 MSK | 29 Mar 25 15:18 MSK |
|            | --driver=docker                |          |      |         |                     |                     |
| docker-env |                                | minikube | mgpu | v1.35.0 | 29 Mar 25 15:21 MSK | 29 Mar 25 15:21 MSK |
| service    | fastapi-service --url          | minikube | mgpu | v1.35.0 | 29 Mar 25 15:33 MSK | 29 Mar 25 15:33 MSK |
| service    | wordpress-service --url        | minikube | mgpu | v1.35.0 | 29 Mar 25 17:31 MSK |                     |
|------------|--------------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/29 15:13:39
Running on machine: mgpu-VirtualBox
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0329 15:13:39.946581  101212 out.go:345] Setting OutFile to fd 1 ...
I0329 15:13:39.946684  101212 out.go:397] isatty.IsTerminal(1) = true
I0329 15:13:39.946687  101212 out.go:358] Setting ErrFile to fd 2...
I0329 15:13:39.946690  101212 out.go:397] isatty.IsTerminal(2) = true
I0329 15:13:39.946847  101212 root.go:338] Updating PATH: /home/mgpu/.minikube/bin
W0329 15:13:39.946933  101212 root.go:314] Error reading config file at /home/mgpu/.minikube/config/config.json: open /home/mgpu/.minikube/config/config.json: no such file or directory
I0329 15:13:39.949217  101212 out.go:352] Setting JSON to false
I0329 15:13:39.950836  101212 start.go:129] hostinfo: {"hostname":"mgpu-VirtualBox","uptime":4249,"bootTime":1743246171,"procs":269,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.0-97-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"128cc634-387b-4a4a-99f2-60e50a91fbcb"}
I0329 15:13:39.950924  101212 start.go:139] virtualization: vbox guest
I0329 15:13:39.953645  101212 out.go:177] 😄  minikube v1.35.0 on Ubuntu 20.04 (vbox/amd64)
W0329 15:13:39.956932  101212 preload.go:293] Failed to list preload files: open /home/mgpu/.minikube/cache/preloaded-tarball: no such file or directory
I0329 15:13:39.957038  101212 notify.go:220] Checking for updates...
I0329 15:13:39.958104  101212 driver.go:394] Setting default libvirt URI to qemu:///system
I0329 15:13:40.306288  101212 docker.go:123] docker version: linux-25.0.3:Docker Engine - Community
I0329 15:13:40.306408  101212 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 15:13:40.666003  101212 info.go:266] docker info: {ID:fb306ab1-20c7-48b9-841e-95c884a56331 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2025-03-29 15:13:40.627955454 +0300 MSK LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-97-generic OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4102148096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:mgpu-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/home/mgpu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6]] Warnings:<nil>}}
I0329 15:13:40.666090  101212 docker.go:318] overlay module found
I0329 15:13:40.667556  101212 out.go:177] ✨  Using the docker driver based on user configuration
I0329 15:13:40.668627  101212 start.go:297] selected driver: docker
I0329 15:13:40.668633  101212 start.go:901] validating driver "docker" against <nil>
I0329 15:13:40.668641  101212 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 15:13:40.669235  101212 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 15:13:40.783819  101212 info.go:266] docker info: {ID:fb306ab1-20c7-48b9-841e-95c884a56331 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2025-03-29 15:13:40.773659026 +0300 MSK LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-97-generic OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4102148096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:mgpu-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/home/mgpu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6]] Warnings:<nil>}}
I0329 15:13:40.783945  101212 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0329 15:13:40.785185  101212 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0329 15:13:40.787589  101212 out.go:177] 📌  Using Docker driver with root privileges
I0329 15:13:40.790632  101212 cni.go:84] Creating CNI manager for ""
I0329 15:13:40.790690  101212 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 15:13:40.790736  101212 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0329 15:13:40.790824  101212 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mgpu:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0329 15:13:40.793789  101212 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0329 15:13:40.796069  101212 cache.go:121] Beginning downloading kic base image for docker with docker
I0329 15:13:40.797892  101212 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0329 15:13:40.801582  101212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0329 15:13:40.801643  101212 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0329 15:13:40.858012  101212 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0329 15:13:40.858217  101212 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0329 15:13:40.858305  101212 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0329 15:13:41.343282  101212 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0329 15:13:41.343315  101212 cache.go:56] Caching tarball of preloaded images
I0329 15:13:41.343787  101212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0329 15:13:41.346378  101212 out.go:177] 💾  Downloading Kubernetes v1.32.0 preload ...
I0329 15:13:41.349196  101212 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0329 15:13:41.557134  101212 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> /home/mgpu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0329 15:14:34.915858  101212 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0329 15:14:34.915951  101212 preload.go:254] verifying checksum of /home/mgpu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0329 15:14:36.212335  101212 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0329 15:14:36.244670  101212 profile.go:143] Saving config to /home/mgpu/.minikube/profiles/minikube/config.json ...
I0329 15:14:36.247861  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/config.json: {Name:mk8107a7a077ced6ec3b7730504592e8dd8d7e0d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:14:52.833130  101212 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0329 15:14:52.833147  101212 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0329 15:16:33.409884  101212 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0329 15:16:33.413413  101212 cache.go:227] Successfully downloaded all kic artifacts
I0329 15:16:33.428971  101212 start.go:360] acquireMachinesLock for minikube: {Name:mkde8dec776cc41debe7bb199c1abf4e1544e868 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0329 15:16:33.435059  101212 start.go:364] duration metric: took 5.001716ms to acquireMachinesLock for "minikube"
I0329 15:16:33.439255  101212 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mgpu:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0329 15:16:33.443660  101212 start.go:125] createHost starting for "" (driver="docker")
I0329 15:16:33.479056  101212 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2048MB) ...
I0329 15:16:33.489925  101212 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0329 15:16:33.489962  101212 client.go:168] LocalClient.Create starting
I0329 15:16:33.494787  101212 main.go:141] libmachine: Creating CA: /home/mgpu/.minikube/certs/ca.pem
I0329 15:16:33.604045  101212 main.go:141] libmachine: Creating client certificate: /home/mgpu/.minikube/certs/cert.pem
I0329 15:16:34.290634  101212 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0329 15:16:34.339414  101212 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0329 15:16:34.339556  101212 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0329 15:16:34.339583  101212 cli_runner.go:164] Run: docker network inspect minikube
W0329 15:16:34.360862  101212 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0329 15:16:34.360888  101212 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0329 15:16:34.360900  101212 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0329 15:16:34.362879  101212 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0329 15:16:34.385179  101212 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0171150b0}
I0329 15:16:34.385231  101212 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0329 15:16:34.385279  101212 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0329 15:16:34.650591  101212 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0329 15:16:34.651496  101212 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0329 15:16:34.651552  101212 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0329 15:16:34.761377  101212 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0329 15:16:34.820561  101212 oci.go:103] Successfully created a docker volume minikube
I0329 15:16:34.820748  101212 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0329 15:16:58.023150  101212 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (23.202352162s)
I0329 15:16:58.023171  101212 oci.go:107] Successfully prepared a docker volume minikube
I0329 15:16:58.024559  101212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0329 15:16:58.025355  101212 kic.go:194] Starting extracting preloaded images to volume ...
I0329 15:16:58.025421  101212 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/mgpu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0329 15:17:08.955849  101212 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/mgpu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (10.930327714s)
I0329 15:17:08.955894  101212 kic.go:203] duration metric: took 10.930540759s to extract preloaded images to volume ...
W0329 15:17:08.958987  101212 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0329 15:17:08.959726  101212 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0329 15:17:09.479618  101212 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2048mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0329 15:17:10.404383  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0329 15:17:10.507509  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:17:10.639978  101212 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0329 15:17:10.954433  101212 oci.go:144] the created container "minikube" has a running status.
I0329 15:17:10.954464  101212 kic.go:225] Creating ssh key for kic: /home/mgpu/.minikube/machines/minikube/id_rsa...
I0329 15:17:11.475712  101212 kic_runner.go:191] docker (temp): /home/mgpu/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0329 15:17:11.510078  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:17:11.543143  101212 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0329 15:17:11.543154  101212 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0329 15:17:11.715998  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:17:11.914718  101212 machine.go:93] provisionDockerMachine start ...
I0329 15:17:11.921973  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:12.012944  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:12.013312  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:12.013320  101212 main.go:141] libmachine: About to run SSH command:
hostname
I0329 15:17:12.016109  101212 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39334->127.0.0.1:32772: read: connection reset by peer
I0329 15:17:15.193751  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0329 15:17:15.193768  101212 ubuntu.go:169] provisioning hostname "minikube"
I0329 15:17:15.193863  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:15.228602  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:15.228967  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:15.228997  101212 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0329 15:17:15.406896  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0329 15:17:15.406976  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:15.454542  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:15.454732  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:15.454743  101212 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0329 15:17:15.610999  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0329 15:17:15.611023  101212 ubuntu.go:175] set auth options {CertDir:/home/mgpu/.minikube CaCertPath:/home/mgpu/.minikube/certs/ca.pem CaPrivateKeyPath:/home/mgpu/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/mgpu/.minikube/machines/server.pem ServerKeyPath:/home/mgpu/.minikube/machines/server-key.pem ClientKeyPath:/home/mgpu/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/mgpu/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/mgpu/.minikube}
I0329 15:17:15.611056  101212 ubuntu.go:177] setting up certificates
I0329 15:17:15.611067  101212 provision.go:84] configureAuth start
I0329 15:17:15.611162  101212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 15:17:15.651617  101212 provision.go:143] copyHostCerts
I0329 15:17:15.652618  101212 exec_runner.go:151] cp: /home/mgpu/.minikube/certs/ca.pem --> /home/mgpu/.minikube/ca.pem (1070 bytes)
I0329 15:17:15.652779  101212 exec_runner.go:151] cp: /home/mgpu/.minikube/certs/cert.pem --> /home/mgpu/.minikube/cert.pem (1115 bytes)
I0329 15:17:15.652837  101212 exec_runner.go:151] cp: /home/mgpu/.minikube/certs/key.pem --> /home/mgpu/.minikube/key.pem (1679 bytes)
I0329 15:17:15.653536  101212 provision.go:117] generating server cert: /home/mgpu/.minikube/machines/server.pem ca-key=/home/mgpu/.minikube/certs/ca.pem private-key=/home/mgpu/.minikube/certs/ca-key.pem org=mgpu.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0329 15:17:16.095705  101212 provision.go:177] copyRemoteCerts
I0329 15:17:16.103524  101212 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0329 15:17:16.103649  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:16.147198  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:17:16.276031  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0329 15:17:16.312470  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0329 15:17:16.352613  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0329 15:17:16.381169  101212 provision.go:87] duration metric: took 770.08553ms to configureAuth
I0329 15:17:16.381186  101212 ubuntu.go:193] setting minikube options for container-runtime
I0329 15:17:16.382468  101212 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0329 15:17:16.382529  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:16.405191  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:16.405450  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:16.405460  101212 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0329 15:17:16.569087  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0329 15:17:16.569100  101212 ubuntu.go:71] root file system type: overlay
I0329 15:17:16.570308  101212 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0329 15:17:16.570369  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:16.590459  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:16.590639  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:16.590690  101212 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0329 15:17:16.755900  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0329 15:17:16.761625  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:16.779878  101212 main.go:141] libmachine: Using SSH client type: native
I0329 15:17:16.780048  101212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 15:17:16.780059  101212 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0329 15:17:18.019827  101212 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-03-29 12:17:16.752390111 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0329 15:17:18.019861  101212 machine.go:96] duration metric: took 6.105116783s to provisionDockerMachine
I0329 15:17:18.019872  101212 client.go:171] duration metric: took 44.529903088s to LocalClient.Create
I0329 15:17:18.019885  101212 start.go:167] duration metric: took 44.529967637s to libmachine.API.Create "minikube"
I0329 15:17:18.019891  101212 start.go:293] postStartSetup for "minikube" (driver="docker")
I0329 15:17:18.019899  101212 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0329 15:17:18.019950  101212 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0329 15:17:18.019982  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:18.045135  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:17:18.156639  101212 ssh_runner.go:195] Run: cat /etc/os-release
I0329 15:17:18.165871  101212 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0329 15:17:18.165900  101212 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0329 15:17:18.165908  101212 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0329 15:17:18.165919  101212 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0329 15:17:18.165933  101212 filesync.go:126] Scanning /home/mgpu/.minikube/addons for local assets ...
I0329 15:17:18.166778  101212 filesync.go:126] Scanning /home/mgpu/.minikube/files for local assets ...
I0329 15:17:18.167609  101212 start.go:296] duration metric: took 147.706232ms for postStartSetup
I0329 15:17:18.168111  101212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 15:17:18.187049  101212 profile.go:143] Saving config to /home/mgpu/.minikube/profiles/minikube/config.json ...
I0329 15:17:18.188215  101212 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0329 15:17:18.188248  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:18.207131  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:17:18.329114  101212 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0329 15:17:18.342784  101212 start.go:128] duration metric: took 44.899102981s to createHost
I0329 15:17:18.342798  101212 start.go:83] releasing machines lock for "minikube", held for 44.907702598s
I0329 15:17:18.342880  101212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 15:17:18.388239  101212 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0329 15:17:18.389088  101212 ssh_runner.go:195] Run: cat /version.json
I0329 15:17:18.389134  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:18.389529  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:17:18.413422  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:17:18.418505  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:17:18.867065  101212 ssh_runner.go:195] Run: systemctl --version
I0329 15:17:18.872333  101212 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0329 15:17:18.879306  101212 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0329 15:17:18.909839  101212 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0329 15:17:18.909897  101212 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0329 15:17:18.977043  101212 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0329 15:17:18.977059  101212 start.go:495] detecting cgroup driver to use...
I0329 15:17:18.977102  101212 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0329 15:17:18.978895  101212 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 15:17:19.001036  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0329 15:17:19.012096  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0329 15:17:19.023422  101212 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0329 15:17:19.023460  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0329 15:17:19.041727  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 15:17:19.059934  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0329 15:17:19.071414  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 15:17:19.083583  101212 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0329 15:17:19.094390  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0329 15:17:19.106539  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0329 15:17:19.119432  101212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0329 15:17:19.135408  101212 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0329 15:17:19.150374  101212 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0329 15:17:19.170603  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:17:19.291245  101212 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0329 15:17:19.443061  101212 start.go:495] detecting cgroup driver to use...
I0329 15:17:19.443101  101212 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0329 15:17:19.443153  101212 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0329 15:17:19.477679  101212 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0329 15:17:19.477755  101212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0329 15:17:19.508423  101212 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 15:17:19.529899  101212 ssh_runner.go:195] Run: which cri-dockerd
I0329 15:17:19.538410  101212 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0329 15:17:19.563192  101212 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0329 15:17:19.601628  101212 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0329 15:17:19.784486  101212 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0329 15:17:19.917349  101212 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0329 15:17:19.917524  101212 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0329 15:17:19.949268  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:17:20.106240  101212 ssh_runner.go:195] Run: sudo systemctl restart docker
I0329 15:17:20.934034  101212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0329 15:17:20.954796  101212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0329 15:17:20.972927  101212 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0329 15:17:21.091551  101212 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0329 15:17:21.209822  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:17:21.330727  101212 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0329 15:17:21.346145  101212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0329 15:17:21.361027  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:17:21.471624  101212 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0329 15:17:21.859020  101212 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0329 15:17:21.859067  101212 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0329 15:17:21.866930  101212 start.go:563] Will wait 60s for crictl version
I0329 15:17:21.866985  101212 ssh_runner.go:195] Run: which crictl
I0329 15:17:21.883211  101212 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0329 15:17:22.100801  101212 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0329 15:17:22.100924  101212 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 15:17:22.254845  101212 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 15:17:22.322252  101212 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0329 15:17:22.324828  101212 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0329 15:17:22.384372  101212 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0329 15:17:22.389925  101212 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0329 15:17:22.415164  101212 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mgpu:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0329 15:17:22.415302  101212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0329 15:17:22.415363  101212 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 15:17:22.444517  101212 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 15:17:22.444531  101212 docker.go:619] Images already preloaded, skipping extraction
I0329 15:17:22.444589  101212 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 15:17:22.488198  101212 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 15:17:22.488211  101212 cache_images.go:84] Images are preloaded, skipping loading
I0329 15:17:22.488220  101212 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0329 15:17:22.489407  101212 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0329 15:17:22.489472  101212 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0329 15:17:22.797121  101212 cni.go:84] Creating CNI manager for ""
I0329 15:17:22.797144  101212 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 15:17:22.797376  101212 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0329 15:17:22.797403  101212 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0329 15:17:22.799318  101212 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0329 15:17:22.799452  101212 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0329 15:17:22.816340  101212 binaries.go:44] Found k8s binaries, skipping transfer
I0329 15:17:22.816430  101212 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0329 15:17:22.828444  101212 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0329 15:17:22.868757  101212 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0329 15:17:22.906696  101212 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0329 15:17:22.933984  101212 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0329 15:17:22.938463  101212 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0329 15:17:22.953128  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:17:23.076705  101212 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0329 15:17:23.097812  101212 certs.go:68] Setting up /home/mgpu/.minikube/profiles/minikube for IP: 192.168.49.2
I0329 15:17:23.097825  101212 certs.go:194] generating shared ca certs ...
I0329 15:17:23.097842  101212 certs.go:226] acquiring lock for ca certs: {Name:mk7006026bf94b2ed398f8e8847b3100e69be6e3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.098001  101212 certs.go:240] generating "minikubeCA" ca cert: /home/mgpu/.minikube/ca.key
I0329 15:17:23.304061  101212 crypto.go:156] Writing cert to /home/mgpu/.minikube/ca.crt ...
I0329 15:17:23.304116  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/ca.crt: {Name:mk356b8558cd7af4aeee9bccdc3bfa67e40cc33c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.304365  101212 crypto.go:164] Writing key to /home/mgpu/.minikube/ca.key ...
I0329 15:17:23.304374  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/ca.key: {Name:mk185985b856842ddca41c94411b2862777d1885 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.304525  101212 certs.go:240] generating "proxyClientCA" ca cert: /home/mgpu/.minikube/proxy-client-ca.key
I0329 15:17:23.526296  101212 crypto.go:156] Writing cert to /home/mgpu/.minikube/proxy-client-ca.crt ...
I0329 15:17:23.526315  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/proxy-client-ca.crt: {Name:mk66a7a43a96c4e3b6f9acd296b4f6055ca516f7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.526521  101212 crypto.go:164] Writing key to /home/mgpu/.minikube/proxy-client-ca.key ...
I0329 15:17:23.526528  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/proxy-client-ca.key: {Name:mk36c87396fa1d505224c0e3c0c064892c54ec44 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.526727  101212 certs.go:256] generating profile certs ...
I0329 15:17:23.526794  101212 certs.go:363] generating signed profile cert for "minikube-user": /home/mgpu/.minikube/profiles/minikube/client.key
I0329 15:17:23.526811  101212 crypto.go:68] Generating cert /home/mgpu/.minikube/profiles/minikube/client.crt with IP's: []
I0329 15:17:23.831906  101212 crypto.go:156] Writing cert to /home/mgpu/.minikube/profiles/minikube/client.crt ...
I0329 15:17:23.831926  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/client.crt: {Name:mk1c682ca4430b22db65610124147a5f89e34d1e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.832234  101212 crypto.go:164] Writing key to /home/mgpu/.minikube/profiles/minikube/client.key ...
I0329 15:17:23.832243  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/client.key: {Name:mkaf117201ca19cf48d984c30ea0e1e20ec4e536 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:23.832332  101212 certs.go:363] generating signed profile cert for "minikube": /home/mgpu/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0329 15:17:23.832345  101212 crypto.go:68] Generating cert /home/mgpu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0329 15:17:24.063469  101212 crypto.go:156] Writing cert to /home/mgpu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0329 15:17:24.063486  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk354ab5795a8a8c55c89ccf292e1e333925ba8d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:24.063835  101212 crypto.go:164] Writing key to /home/mgpu/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0329 15:17:24.063849  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk24fa57d516b399f23b4e8a6c9b21b4c524c04d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:24.063989  101212 certs.go:381] copying /home/mgpu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/mgpu/.minikube/profiles/minikube/apiserver.crt
I0329 15:17:24.064977  101212 certs.go:385] copying /home/mgpu/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/mgpu/.minikube/profiles/minikube/apiserver.key
I0329 15:17:24.065077  101212 certs.go:363] generating signed profile cert for "aggregator": /home/mgpu/.minikube/profiles/minikube/proxy-client.key
I0329 15:17:24.065101  101212 crypto.go:68] Generating cert /home/mgpu/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0329 15:17:24.673151  101212 crypto.go:156] Writing cert to /home/mgpu/.minikube/profiles/minikube/proxy-client.crt ...
I0329 15:17:24.673170  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/proxy-client.crt: {Name:mkc89f3c5a846bc51c3f645c2f0d7bc0a77e03dc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:24.673578  101212 crypto.go:164] Writing key to /home/mgpu/.minikube/profiles/minikube/proxy-client.key ...
I0329 15:17:24.673589  101212 lock.go:35] WriteFile acquiring /home/mgpu/.minikube/profiles/minikube/proxy-client.key: {Name:mk3652aa008d6e7d11bb7a3d587241cde022b82c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:17:24.673908  101212 certs.go:484] found cert: /home/mgpu/.minikube/certs/ca-key.pem (1679 bytes)
I0329 15:17:24.673944  101212 certs.go:484] found cert: /home/mgpu/.minikube/certs/ca.pem (1070 bytes)
I0329 15:17:24.673970  101212 certs.go:484] found cert: /home/mgpu/.minikube/certs/cert.pem (1115 bytes)
I0329 15:17:24.675439  101212 certs.go:484] found cert: /home/mgpu/.minikube/certs/key.pem (1679 bytes)
I0329 15:17:24.694517  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0329 15:17:24.744714  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0329 15:17:24.783142  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0329 15:17:24.837273  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0329 15:17:24.872052  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0329 15:17:24.913872  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0329 15:17:24.950064  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0329 15:17:24.984557  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0329 15:17:25.028232  101212 ssh_runner.go:362] scp /home/mgpu/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0329 15:17:25.065142  101212 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0329 15:17:25.092081  101212 ssh_runner.go:195] Run: openssl version
I0329 15:17:25.116952  101212 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0329 15:17:25.152549  101212 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0329 15:17:25.159514  101212 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 29 12:17 /usr/share/ca-certificates/minikubeCA.pem
I0329 15:17:25.159635  101212 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0329 15:17:25.169786  101212 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0329 15:17:25.183529  101212 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0329 15:17:25.190163  101212 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0329 15:17:25.190229  101212 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mgpu:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0329 15:17:25.190418  101212 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0329 15:17:25.228242  101212 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0329 15:17:25.242896  101212 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0329 15:17:25.255231  101212 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0329 15:17:25.255275  101212 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0329 15:17:25.266425  101212 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0329 15:17:25.266435  101212 kubeadm.go:157] found existing configuration files:

I0329 15:17:25.266474  101212 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0329 15:17:25.277278  101212 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0329 15:17:25.277324  101212 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0329 15:17:25.288572  101212 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0329 15:17:25.303772  101212 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0329 15:17:25.303817  101212 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0329 15:17:25.323912  101212 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0329 15:17:25.335652  101212 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0329 15:17:25.335700  101212 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0329 15:17:25.348412  101212 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0329 15:17:25.360755  101212 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0329 15:17:25.360802  101212 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0329 15:17:25.372313  101212 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0329 15:17:25.699434  101212 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0329 15:17:25.699934  101212 kubeadm.go:310] [preflight] Running pre-flight checks
I0329 15:17:25.773119  101212 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0329 15:17:25.774119  101212 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-97-generic[0m
I0329 15:17:25.774161  101212 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0329 15:17:25.774211  101212 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0329 15:17:25.774258  101212 kubeadm.go:310] [0;37mCGROUPS_CPUACCT[0m: [0;32menabled[0m
I0329 15:17:25.774304  101212 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0329 15:17:25.774366  101212 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0329 15:17:25.774415  101212 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0329 15:17:25.774473  101212 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0329 15:17:25.774517  101212 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0329 15:17:25.774563  101212 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0329 15:17:25.774608  101212 kubeadm.go:310] [0;37mCGROUPS_BLKIO[0m: [0;32menabled[0m
I0329 15:17:25.877581  101212 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0329 15:17:25.877796  101212 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0329 15:17:25.877939  101212 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0329 15:17:25.916252  101212 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0329 15:17:25.920570  101212 out.go:235]     ▪ Generating certificates and keys ...
I0329 15:17:25.926024  101212 kubeadm.go:310] [certs] Using existing ca certificate authority
I0329 15:17:25.926126  101212 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0329 15:17:26.403392  101212 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0329 15:17:26.774870  101212 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0329 15:17:27.203923  101212 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0329 15:17:27.476947  101212 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0329 15:17:27.679544  101212 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0329 15:17:27.680045  101212 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0329 15:17:27.801786  101212 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0329 15:17:27.801916  101212 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0329 15:17:28.007165  101212 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0329 15:17:28.180654  101212 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0329 15:17:28.376081  101212 kubeadm.go:310] [certs] Generating "sa" key and public key
I0329 15:17:28.376487  101212 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0329 15:17:28.879488  101212 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0329 15:17:29.183902  101212 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0329 15:17:29.404987  101212 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0329 15:17:29.530857  101212 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0329 15:17:30.559676  101212 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0329 15:17:30.560622  101212 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0329 15:17:30.563637  101212 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0329 15:17:32.165003  101212 out.go:235]     ▪ Booting up control plane ...
I0329 15:17:32.165644  101212 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0329 15:17:32.165758  101212 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0329 15:17:32.165972  101212 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0329 15:17:32.166132  101212 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0329 15:17:32.181678  101212 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0329 15:17:32.183408  101212 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0329 15:17:32.386131  101212 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0329 15:17:32.386251  101212 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0329 15:17:33.901932  101212 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.511138317s
I0329 15:17:33.902020  101212 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0329 15:17:57.902441  101212 kubeadm.go:310] [api-check] The API server is healthy after 24.004996439s
I0329 15:17:57.923903  101212 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0329 15:17:57.952441  101212 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0329 15:17:57.988430  101212 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0329 15:17:57.988722  101212 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0329 15:17:58.030965  101212 kubeadm.go:310] [bootstrap-token] Using token: 778klc.l7092rnqm80aj89v
I0329 15:17:58.033529  101212 out.go:235]     ▪ Configuring RBAC rules ...
I0329 15:17:58.033657  101212 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0329 15:17:58.053099  101212 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0329 15:17:58.071650  101212 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0329 15:17:58.078788  101212 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0329 15:17:58.086291  101212 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0329 15:17:58.098661  101212 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0329 15:17:58.311234  101212 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0329 15:17:58.777947  101212 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0329 15:17:59.314454  101212 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0329 15:17:59.315383  101212 kubeadm.go:310] 
I0329 15:17:59.315451  101212 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0329 15:17:59.315454  101212 kubeadm.go:310] 
I0329 15:17:59.315518  101212 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0329 15:17:59.315521  101212 kubeadm.go:310] 
I0329 15:17:59.315541  101212 kubeadm.go:310]   mkdir -p $HOME/.kube
I0329 15:17:59.315589  101212 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0329 15:17:59.315653  101212 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0329 15:17:59.315656  101212 kubeadm.go:310] 
I0329 15:17:59.315713  101212 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0329 15:17:59.315718  101212 kubeadm.go:310] 
I0329 15:17:59.315773  101212 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0329 15:17:59.315777  101212 kubeadm.go:310] 
I0329 15:17:59.315854  101212 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0329 15:17:59.315984  101212 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0329 15:17:59.316067  101212 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0329 15:17:59.316071  101212 kubeadm.go:310] 
I0329 15:17:59.316172  101212 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0329 15:17:59.316286  101212 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0329 15:17:59.316291  101212 kubeadm.go:310] 
I0329 15:17:59.316430  101212 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 778klc.l7092rnqm80aj89v \
I0329 15:17:59.316553  101212 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8b854603264d704c7442f71c8a75db63784b83e3bf669035d3c7191c54d888a3 \
I0329 15:17:59.316577  101212 kubeadm.go:310] 	--control-plane 
I0329 15:17:59.316580  101212 kubeadm.go:310] 
I0329 15:17:59.316710  101212 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0329 15:17:59.316714  101212 kubeadm.go:310] 
I0329 15:17:59.316782  101212 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 778klc.l7092rnqm80aj89v \
I0329 15:17:59.316871  101212 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8b854603264d704c7442f71c8a75db63784b83e3bf669035d3c7191c54d888a3 
I0329 15:17:59.320856  101212 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0329 15:17:59.320984  101212 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0329 15:17:59.321159  101212 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-97-generic\n", err: exit status 1
I0329 15:17:59.321244  101212 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0329 15:17:59.321258  101212 cni.go:84] Creating CNI manager for ""
I0329 15:17:59.321271  101212 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 15:17:59.323806  101212 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0329 15:17:59.326073  101212 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0329 15:17:59.370964  101212 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0329 15:17:59.409833  101212 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0329 15:17:59.409860  101212 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0329 15:17:59.409925  101212 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_29T15_17_59_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0329 15:17:59.461463  101212 ops.go:34] apiserver oom_adj: -16
I0329 15:18:00.240616  101212 kubeadm.go:1113] duration metric: took 830.804087ms to wait for elevateKubeSystemPrivileges
I0329 15:18:00.240649  101212 kubeadm.go:394] duration metric: took 35.050424523s to StartCluster
I0329 15:18:00.240672  101212 settings.go:142] acquiring lock: {Name:mk6f2e4d6eca988c1d64aafcdcaf78bfd167d37c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:18:00.240798  101212 settings.go:150] Updating kubeconfig:  /home/mgpu/.kube/config
I0329 15:18:00.241369  101212 lock.go:35] WriteFile acquiring /home/mgpu/.kube/config: {Name:mkb13cf1ce3b035954b087c50ccc1a1f6eb8d4fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 15:18:00.241712  101212 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0329 15:18:00.242259  101212 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0329 15:18:00.242449  101212 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0329 15:18:00.243919  101212 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0329 15:18:00.243988  101212 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0329 15:18:00.244002  101212 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0329 15:18:00.244029  101212 host.go:66] Checking if "minikube" exists ...
I0329 15:18:00.244026  101212 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0329 15:18:00.244040  101212 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0329 15:18:00.259599  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:18:00.259945  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:18:00.263450  101212 out.go:177] 🔎  Verifying Kubernetes components...
I0329 15:18:00.282595  101212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 15:18:00.480208  101212 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0329 15:18:00.501833  101212 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0329 15:18:00.501846  101212 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0329 15:18:00.501904  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:18:00.579521  101212 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0329 15:18:00.579568  101212 host.go:66] Checking if "minikube" exists ...
I0329 15:18:00.579888  101212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 15:18:00.618669  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:18:00.630122  101212 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0329 15:18:00.630187  101212 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0329 15:18:00.688209  101212 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0329 15:18:00.688222  101212 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0329 15:18:00.688272  101212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 15:18:00.732498  101212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mgpu/.minikube/machines/minikube/id_rsa Username:docker}
I0329 15:18:00.849647  101212 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0329 15:18:00.953635  101212 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0329 15:18:01.100214  101212 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0329 15:18:01.101584  101212 api_server.go:52] waiting for apiserver process to appear ...
I0329 15:18:01.101621  101212 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 15:18:01.479879  101212 api_server.go:72] duration metric: took 1.237401831s to wait for apiserver process to appear ...
I0329 15:18:01.479891  101212 api_server.go:88] waiting for apiserver healthz status ...
I0329 15:18:01.479913  101212 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0329 15:18:01.506099  101212 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0329 15:18:01.508995  101212 api_server.go:141] control plane version: v1.32.0
I0329 15:18:01.509051  101212 api_server.go:131] duration metric: took 29.151409ms to wait for apiserver health ...
I0329 15:18:01.509094  101212 system_pods.go:43] waiting for kube-system pods to appear ...
I0329 15:18:01.524899  101212 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0329 15:18:01.527129  101212 addons.go:514] duration metric: took 1.284353959s for enable addons: enabled=[storage-provisioner default-storageclass]
I0329 15:18:01.530817  101212 system_pods.go:59] 5 kube-system pods found
I0329 15:18:01.530854  101212 system_pods.go:61] "etcd-minikube" [3e2f91e4-e2f6-456b-a1b9-e4861504756f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0329 15:18:01.530864  101212 system_pods.go:61] "kube-apiserver-minikube" [671e8a5c-74bd-4e4c-af02-996193f4992c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0329 15:18:01.530869  101212 system_pods.go:61] "kube-controller-manager-minikube" [73b466f9-107f-41aa-9a3d-50e83967857b] Running
I0329 15:18:01.530873  101212 system_pods.go:61] "kube-scheduler-minikube" [1ef602f2-4602-4c53-a27f-0795c9e9ab75] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0329 15:18:01.530876  101212 system_pods.go:61] "storage-provisioner" [dd8d2b9f-d849-478e-a643-aa05301713d5] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0329 15:18:01.530882  101212 system_pods.go:74] duration metric: took 21.782472ms to wait for pod list to return data ...
I0329 15:18:01.530892  101212 kubeadm.go:582] duration metric: took 1.288422848s to wait for: map[apiserver:true system_pods:true]
I0329 15:18:01.530905  101212 node_conditions.go:102] verifying NodePressure condition ...
I0329 15:18:01.576420  101212 node_conditions.go:122] node storage ephemeral capacity is 25252644Ki
I0329 15:18:01.576440  101212 node_conditions.go:123] node cpu capacity is 2
I0329 15:18:01.576482  101212 node_conditions.go:105] duration metric: took 45.544015ms to run NodePressure ...
I0329 15:18:01.576498  101212 start.go:241] waiting for startup goroutines ...
I0329 15:18:01.606312  101212 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0329 15:18:01.606332  101212 start.go:246] waiting for cluster config update ...
I0329 15:18:01.606346  101212 start.go:255] writing updated cluster config ...
I0329 15:18:01.606666  101212 ssh_runner.go:195] Run: rm -f paused
I0329 15:18:04.791881  101212 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0329 15:18:04.810730  101212 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 29 12:17:20 minikube dockerd[1316]: time="2025-03-29T12:17:20.882581507Z" level=info msg="Loading containers: done."
Mar 29 12:17:20 minikube dockerd[1316]: time="2025-03-29T12:17:20.897047149Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Mar 29 12:17:20 minikube dockerd[1316]: time="2025-03-29T12:17:20.897221225Z" level=info msg="Daemon has completed initialization"
Mar 29 12:17:20 minikube dockerd[1316]: time="2025-03-29T12:17:20.931855340Z" level=info msg="API listen on /var/run/docker.sock"
Mar 29 12:17:20 minikube dockerd[1316]: time="2025-03-29T12:17:20.932122281Z" level=info msg="API listen on [::]:2376"
Mar 29 12:17:20 minikube systemd[1]: Started Docker Application Container Engine.
Mar 29 12:17:21 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Start docker client with request timeout 0s"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Hairpin mode is set to hairpin-veth"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Loaded network plugin cni"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Docker cri networking managed by network plugin cni"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Setting cgroupDriver cgroupfs"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 29 12:17:21 minikube cri-dockerd[1584]: time="2025-03-29T12:17:21Z" level=info msg="Start cri-dockerd grpc backend"
Mar 29 12:17:21 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 29 12:17:41 minikube cri-dockerd[1584]: time="2025-03-29T12:17:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1c94749c424cfb97f122b6222c9833a2d9ba24709efe9a6cdfe6974ac53336b1/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Mar 29 12:17:41 minikube cri-dockerd[1584]: time="2025-03-29T12:17:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a1a5b18d088bb0dda748a0e0ace9f3ecc50a3c24eccf8060cf44280f7e3a33c/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Mar 29 12:17:41 minikube cri-dockerd[1584]: time="2025-03-29T12:17:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d9cef2f1e5b8466630bbea58909817f45fc027a962c9d6fb6eb8346f91a9fd6c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 29 12:17:41 minikube cri-dockerd[1584]: time="2025-03-29T12:17:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b1678cee33194a3cb5b6f50cec3c76269795a34877e8f7e7d1e2b1977db43cfa/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 29 12:18:05 minikube cri-dockerd[1584]: time="2025-03-29T12:18:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3ba7df4f0b24d817395496e5abe60f374b1666b035b7600294123871489deaaf/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 29 12:18:05 minikube cri-dockerd[1584]: time="2025-03-29T12:18:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8300a0844ea80705fd64002ec56314000daa5f0ad7e04a4233ac65023869ec26/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Mar 29 12:18:08 minikube cri-dockerd[1584]: time="2025-03-29T12:18:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8aff3c576a40958a92e8e2c807511fe4ef0651a2f0e6fd7994b328e9c1dc7707/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 29 12:18:09 minikube cri-dockerd[1584]: time="2025-03-29T12:18:09Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 29 12:18:37 minikube dockerd[1316]: time="2025-03-29T12:18:37.920994238Z" level=info msg="ignoring event" container=7ff5b19bd25da25d71860b39372bf19509e3955edd183bb495a40334ce5fcb89 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:23:32 minikube dockerd[1316]: time="2025-03-29T12:23:32.438134689Z" level=info msg="ignoring event" container=5075443048c7ef0500376f5b891f1f6576a1fa16712b2fafc772bfed69e2c861 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:26:06 minikube cri-dockerd[1584]: time="2025-03-29T12:26:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09aae8aa13ab092d4ce657b5fef22894cff94534586deb05416a34c543027cd1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 12:26:06 minikube cri-dockerd[1584]: time="2025-03-29T12:26:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85978c9ec1af2a3a0931fb7be3211653279ec616f210355048fab571d5fbe8d7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 12:26:09 minikube cri-dockerd[1584]: time="2025-03-29T12:26:09Z" level=info msg="Stop pulling image busybox:latest: Status: Downloaded newer image for busybox:latest"
Mar 29 12:26:10 minikube dockerd[1316]: time="2025-03-29T12:26:10.139117742Z" level=info msg="ignoring event" container=cc790353b87347a4a22a57a61e356ba4f45512eebb757a51d58610c7c32dcad7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:26:11 minikube cri-dockerd[1584]: time="2025-03-29T12:26:11Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
Mar 29 12:26:11 minikube dockerd[1316]: time="2025-03-29T12:26:11.754030036Z" level=info msg="ignoring event" container=e2c5f4834170aa6468ae25d1fec3063b5fa91bb0ea9a3a999efead3c09fbdf1b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:28:36 minikube dockerd[1316]: time="2025-03-29T12:28:36.297987704Z" level=info msg="ignoring event" container=948eb5d87405c04bd886258692c22962589074b68e84572308f1752195156365 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:28:36 minikube dockerd[1316]: time="2025-03-29T12:28:36.316781398Z" level=info msg="ignoring event" container=8e00947b9481f5532cc6ebeb5bb54df91a108300e219519d6ec85ce0ac29ed1f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 12:29:13 minikube cri-dockerd[1584]: time="2025-03-29T12:29:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/809a34ca1f1418a9c425835fd44b1d25ef26d80cee708e42fe911e5c700cdf7d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 12:29:21 minikube cri-dockerd[1584]: time="2025-03-29T12:29:21Z" level=info msg="Stop pulling image redis:latest: Status: Downloaded newer image for redis:latest"
Mar 29 14:16:41 minikube cri-dockerd[1584]: time="2025-03-29T14:16:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/663004a822604dafd4a9ffefa401e236a0f691f5225885feb237f877964c8e39/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 14:16:58 minikube dockerd[1316]: time="2025-03-29T14:16:58.084078163Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:16:58 minikube dockerd[1316]: time="2025-03-29T14:16:58.085259099Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:16:58 minikube dockerd[1316]: time="2025-03-29T14:16:58.090341492Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:17:29 minikube dockerd[1316]: time="2025-03-29T14:17:29.945968297Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:17:29 minikube dockerd[1316]: time="2025-03-29T14:17:29.946065310Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:17:29 minikube dockerd[1316]: time="2025-03-29T14:17:29.950770799Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:18:11 minikube dockerd[1316]: time="2025-03-29T14:18:11.960970392Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:18:11 minikube dockerd[1316]: time="2025-03-29T14:18:11.961015176Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:18:11 minikube dockerd[1316]: time="2025-03-29T14:18:11.963903759Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:19:12 minikube dockerd[1316]: time="2025-03-29T14:19:12.967560485Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:19:12 minikube dockerd[1316]: time="2025-03-29T14:19:12.967773149Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:19:12 minikube dockerd[1316]: time="2025-03-29T14:19:12.972033801Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:20:59 minikube dockerd[1316]: time="2025-03-29T14:20:59.950994993Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:20:59 minikube dockerd[1316]: time="2025-03-29T14:20:59.951036890Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:20:59 minikube dockerd[1316]: time="2025-03-29T14:20:59.953975882Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:24:09 minikube dockerd[1316]: time="2025-03-29T14:24:09.955913438Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:24:09 minikube dockerd[1316]: time="2025-03-29T14:24:09.956045867Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:24:09 minikube dockerd[1316]: time="2025-03-29T14:24:09.963638105Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:29:34 minikube dockerd[1316]: time="2025-03-29T14:29:34.955154475Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:29:34 minikube dockerd[1316]: time="2025-03-29T14:29:34.956367338Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"
Mar 29 14:29:34 minikube dockerd[1316]: time="2025-03-29T14:29:34.960350665Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving"


==> container status <==
CONTAINER           IMAGE                                                                             CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
5d05c698bd26b       redis@sha256:bd41d55aae1ecff61b2fafd0d66761223fe94a60373eb6bb781cfbb570a84079     2 hours ago         Running             redis                     0                   809a34ca1f141       redis-deployment-748ffbc5f5-kzqb6
d89bd5fe0dac8       75243567a0075                                                                     2 hours ago         Running             fastapi                   1                   09aae8aa13ab0       fastapi-deployment-cf4dc69bc-5qdv5
8973ec1d82538       75243567a0075                                                                     2 hours ago         Running             fastapi                   1                   85978c9ec1af2       fastapi-deployment-cf4dc69bc-lw2qc
8e00947b9481f       75243567a0075                                                                     2 hours ago         Exited              fastapi                   0                   09aae8aa13ab0       fastapi-deployment-cf4dc69bc-5qdv5
e2c5f4834170a       busybox@sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f   2 hours ago         Exited              init-myservice            0                   09aae8aa13ab0       fastapi-deployment-cf4dc69bc-5qdv5
948eb5d87405c       75243567a0075                                                                     2 hours ago         Exited              fastapi                   0                   85978c9ec1af2       fastapi-deployment-cf4dc69bc-lw2qc
cc790353b8734       busybox@sha256:37f7b378a29ceb4c551b1b5582e27747b855bbfaa73fa11914fe0df028dc581f   2 hours ago         Exited              init-myservice            0                   85978c9ec1af2       fastapi-deployment-cf4dc69bc-lw2qc
39af22247098a       6e38f40d628db                                                                     2 hours ago         Running             storage-provisioner       2                   8300a0844ea80       storage-provisioner
5075443048c7e       6e38f40d628db                                                                     2 hours ago         Exited              storage-provisioner       1                   8300a0844ea80       storage-provisioner
089688109d349       c69fa2e9cbf5f                                                                     2 hours ago         Running             coredns                   0                   8aff3c576a409       coredns-668d6bf9bc-gzcbd
2117abf446cf1       040f9f8aac8cd                                                                     2 hours ago         Running             kube-proxy                0                   3ba7df4f0b24d       kube-proxy-z2x5x
f02c006d73589       a9e7e6b294baf                                                                     2 hours ago         Running             etcd                      0                   7a1a5b18d088b       etcd-minikube
aed2f89035832       a389e107f4ff1                                                                     2 hours ago         Running             kube-scheduler            0                   b1678cee33194       kube-scheduler-minikube
5b6d85df3b691       c2e17b8d0f4a3                                                                     2 hours ago         Running             kube-apiserver            0                   d9cef2f1e5b84       kube-apiserver-minikube
8bf7fc65f3ed9       8cab3d2a8bd0f                                                                     2 hours ago         Running             kube-controller-manager   0                   1c94749c424cf       kube-controller-manager-minikube


==> coredns [089688109d34] <==
[INFO] 10.244.0.3:58860 - 12961 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000107318s
[INFO] 10.244.0.3:58860 - 16063 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000168002s
[INFO] 10.244.0.3:51340 - 33019 "A IN redis-service. udp 31 false 512" SERVFAIL qr,rd 31 0.00113245s
[INFO] 10.244.0.3:51340 - 27896 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,rd 31 0.001315375s
[INFO] 10.244.0.3:51340 - 33019 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000046398s
[INFO] 10.244.0.3:51340 - 27896 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000092256s
[INFO] 10.244.0.4:52195 - 51936 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000156689s
[INFO] 10.244.0.4:52195 - 62946 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000369067s
[INFO] 10.244.0.4:48411 - 20623 "A IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000083857s
[INFO] 10.244.0.4:48411 - 3726 "AAAA IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000153537s
[INFO] 10.244.0.4:52314 - 30211 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000140375s
[INFO] 10.244.0.4:52314 - 59398 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000056382s
[INFO] 10.244.0.4:39426 - 58937 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000041648s
[INFO] 10.244.0.4:39426 - 2618 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000026093s
[INFO] 10.244.0.4:39426 - 58937 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000022367s
[INFO] 10.244.0.4:39426 - 2618 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000030701s
[INFO] 10.244.0.3:45066 - 20158 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000228563s
[INFO] 10.244.0.3:45066 - 63922 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000242032s
[INFO] 10.244.0.3:46367 - 13693 "A IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00012972s
[INFO] 10.244.0.3:46367 - 5240 "AAAA IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000245349s
[INFO] 10.244.0.3:56523 - 17064 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00009399s
[INFO] 10.244.0.3:56523 - 59055 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000306932s
[INFO] 10.244.0.3:58860 - 30118 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,rd 31 0.001427099s
[INFO] 10.244.0.3:58860 - 11176 "A IN redis-service. udp 31 false 512" SERVFAIL qr,rd 31 0.001531548s
[INFO] 10.244.0.3:58860 - 30118 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000092329s
[INFO] 10.244.0.3:58860 - 11176 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000226617s
[INFO] 10.244.0.4:53589 - 18900 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000216567s
[INFO] 10.244.0.4:53589 - 47826 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000169323s
[INFO] 10.244.0.4:45891 - 10977 "A IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000078676s
[INFO] 10.244.0.4:45891 - 59116 "AAAA IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000161126s
[INFO] 10.244.0.4:48529 - 21930 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000075055s
[INFO] 10.244.0.4:48529 - 48812 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000119576s
[INFO] 10.244.0.4:42891 - 25866 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.00006521s
[INFO] 10.244.0.4:42891 - 7691 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000030423s
[INFO] 10.244.0.4:42891 - 25866 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000036134s
[INFO] 10.244.0.4:42891 - 7691 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000024055s
[INFO] 10.244.0.4:59088 - 29329 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000172946s
[INFO] 10.244.0.4:59088 - 21663 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.00017422s
[INFO] 10.244.0.4:46406 - 47559 "AAAA IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000072602s
[INFO] 10.244.0.4:46406 - 11969 "A IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000832666s
[INFO] 10.244.0.4:47583 - 20889 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000086574s
[INFO] 10.244.0.4:47583 - 42651 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000116704s
[INFO] 10.244.0.4:32906 - 30674 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000036961s
[INFO] 10.244.0.4:32906 - 38621 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000090079s
[INFO] 10.244.0.4:32906 - 30674 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000044709s
[INFO] 10.244.0.4:32906 - 38621 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000082738s
[INFO] 10.244.0.3:38109 - 56023 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000235899s
[INFO] 10.244.0.3:38109 - 40913 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000261762s
[INFO] 10.244.0.3:55579 - 45308 "A IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000173226s
[INFO] 10.244.0.3:55579 - 35326 "AAAA IN redis-service.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000077833s
[INFO] 10.244.0.3:57840 - 11398 "A IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.00013506s
[INFO] 10.244.0.3:57840 - 28036 "AAAA IN redis-service.cluster.local. udp 45 false 512" NXDOMAIN qr,aa,rd 138 0.000078643s
[INFO] 10.244.0.3:47739 - 64055 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000100891s
[INFO] 10.244.0.3:47739 - 62774 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000038487s
[INFO] 10.244.0.3:47739 - 64055 "A IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.0000464s
[INFO] 10.244.0.3:47739 - 62774 "AAAA IN redis-service. udp 31 false 512" SERVFAIL qr,aa,rd 31 0.000137132s
[INFO] 10.244.0.3:37622 - 2345 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.001271129s
[INFO] 10.244.0.3:37622 - 811 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.001026612s
[INFO] 10.244.0.4:45952 - 1334 "AAAA IN redis-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000124397s
[INFO] 10.244.0.4:45952 - 19256 "A IN redis-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000093018s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_29T15_17_59_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 29 Mar 2025 12:17:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 29 Mar 2025 14:32:13 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 29 Mar 2025 14:27:27 +0000   Sat, 29 Mar 2025 12:17:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 29 Mar 2025 14:27:27 +0000   Sat, 29 Mar 2025 12:17:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 29 Mar 2025 14:27:27 +0000   Sat, 29 Mar 2025 12:17:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 29 Mar 2025 14:27:27 +0000   Sat, 29 Mar 2025 12:17:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  25252644Ki
  hugepages-2Mi:      0
  memory:             4006004Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  25252644Ki
  hugepages-2Mi:      0
  memory:             4006004Ki
  pods:               110
System Info:
  Machine ID:                 5f6a6b67d8e149a7a545f930872bd5ec
  System UUID:                287c0498-5f4a-4e94-8c6b-5e7a67a9387e
  Boot ID:                    60dae172-6970-4995-b1b8-35f5a120fa32
  Kernel Version:             5.15.0-97-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                     ------------  ----------  ---------------  -------------  ---
  default                     fastapi-deployment-cf4dc69bc-5qdv5       0 (0%)        0 (0%)      0 (0%)           0 (0%)         126m
  default                     fastapi-deployment-cf4dc69bc-lw2qc       0 (0%)        0 (0%)      0 (0%)           0 (0%)         126m
  default                     redis-deployment-748ffbc5f5-kzqb6        0 (0%)        0 (0%)      0 (0%)           0 (0%)         123m
  default                     wordpress-deployment-594ffdd65f-sxrjh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m
  kube-system                 coredns-668d6bf9bc-gzcbd                 100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     134m
  kube-system                 etcd-minikube                            100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         134m
  kube-system                 kube-apiserver-minikube                  250m (12%)    0 (0%)      0 (0%)           0 (0%)         134m
  kube-system                 kube-controller-manager-minikube         200m (10%)    0 (0%)      0 (0%)           0 (0%)         134m
  kube-system                 kube-proxy-z2x5x                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         134m
  kube-system                 kube-scheduler-minikube                  100m (5%)     0 (0%)      0 (0%)           0 (0%)         134m
  kube-system                 storage-provisioner                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         134m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Mar29 11:02] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.274328] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000071] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +1.560735] [drm:vmw_host_printf [vmwgfx]] *ERROR* Failed to send host log message.
[  +5.022428] kauditd_printk_skb: 32 callbacks suppressed
[Mar29 11:14] kauditd_printk_skb: 4 callbacks suppressed
[Mar29 11:29] kauditd_printk_skb: 19 callbacks suppressed
[Mar29 11:53] show_signal_msg: 20 callbacks suppressed
[Mar29 12:17] tmpfs: Unknown parameter 'noswap'
[ +25.063856] tmpfs: Unknown parameter 'noswap'
[Mar29 13:47] hrtimer: interrupt took 8502206 ns


==> etcd [f02c006d7358] <==
{"level":"info","ts":"2025-03-29T14:22:02.915869Z","caller":"traceutil/trace.go:171","msg":"trace[2021888682] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:6442; }","duration":"256.750112ms","start":"2025-03-29T14:22:02.659087Z","end":"2025-03-29T14:22:02.915838Z","steps":["trace[2021888682] 'agreement among raft nodes before linearized reading'  (duration: 256.058258ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:02.915677Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"246.945342ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:02.916663Z","caller":"traceutil/trace.go:171","msg":"trace[1189441089] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6442; }","duration":"247.930637ms","start":"2025-03-29T14:22:02.668709Z","end":"2025-03-29T14:22:02.916640Z","steps":["trace[1189441089] 'agreement among raft nodes before linearized reading'  (duration: 246.692709ms)"],"step_count":1}
{"level":"info","ts":"2025-03-29T14:22:03.091706Z","caller":"traceutil/trace.go:171","msg":"trace[316004298] transaction","detail":"{read_only:false; response_revision:6443; number_of_response:1; }","duration":"164.739387ms","start":"2025-03-29T14:22:02.926907Z","end":"2025-03-29T14:22:03.091646Z","steps":["trace[316004298] 'process raft request'  (duration: 164.581351ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:09.914774Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.016816323s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036224692514012 > lease_revoke:<id:70cc95e1d64dec8e>","response":"size:29"}
{"level":"info","ts":"2025-03-29T14:22:09.914979Z","caller":"traceutil/trace.go:171","msg":"trace[596833852] linearizableReadLoop","detail":"{readStateIndex:7992; appliedIndex:7991; }","duration":"1.270366407s","start":"2025-03-29T14:22:08.644594Z","end":"2025-03-29T14:22:09.914961Z","steps":["trace[596833852] 'read index received'  (duration: 253.3331ms)","trace[596833852] 'applied index is now lower than readState.Index'  (duration: 1.0170318s)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:09.916016Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.271407596s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:09.916369Z","caller":"traceutil/trace.go:171","msg":"trace[1467584169] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6447; }","duration":"1.271788s","start":"2025-03-29T14:22:08.644565Z","end":"2025-03-29T14:22:09.916353Z","steps":["trace[1467584169] 'agreement among raft nodes before linearized reading'  (duration: 1.271393906s)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:09.916802Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:08.644548Z","time spent":"1.272235742s","remote":"127.0.0.1:57194","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-03-29T14:22:09.917191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.247022165s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:09.917332Z","caller":"traceutil/trace.go:171","msg":"trace[115903700] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6447; }","duration":"1.247548362s","start":"2025-03-29T14:22:08.669767Z","end":"2025-03-29T14:22:09.917315Z","steps":["trace[115903700] 'agreement among raft nodes before linearized reading'  (duration: 1.246631776s)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:09.917886Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"754.519224ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2025-03-29T14:22:09.918160Z","caller":"traceutil/trace.go:171","msg":"trace[722618675] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:6447; }","duration":"754.772581ms","start":"2025-03-29T14:22:09.163316Z","end":"2025-03-29T14:22:09.918089Z","steps":["trace[722618675] 'agreement among raft nodes before linearized reading'  (duration: 754.374824ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:09.918248Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:09.163294Z","time spent":"754.93898ms","remote":"127.0.0.1:57336","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":625,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-03-29T14:22:09.918882Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"926.713948ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:09.918978Z","caller":"traceutil/trace.go:171","msg":"trace[1883411571] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:6447; }","duration":"926.835293ms","start":"2025-03-29T14:22:08.992133Z","end":"2025-03-29T14:22:09.918968Z","steps":["trace[1883411571] 'agreement among raft nodes before linearized reading'  (duration: 926.703852ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:09.919047Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:08.992119Z","time spent":"926.917393ms","remote":"127.0.0.1:57680","response type":"/etcdserverpb.KV/Range","request count":0,"request size":94,"response count":0,"response size":29,"request content":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true "}
{"level":"warn","ts":"2025-03-29T14:22:10.425185Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128036224692514014,"retry-timeout":"500ms"}
{"level":"info","ts":"2025-03-29T14:22:10.442656Z","caller":"traceutil/trace.go:171","msg":"trace[39038911] transaction","detail":"{read_only:false; response_revision:6448; number_of_response:1; }","duration":"522.646395ms","start":"2025-03-29T14:22:09.919973Z","end":"2025-03-29T14:22:10.442620Z","steps":["trace[39038911] 'process raft request'  (duration: 522.295278ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:10.442868Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:09.919951Z","time spent":"522.802035ms","remote":"127.0.0.1:57418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:6439 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-03-29T14:22:11.811569Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"877.154843ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036224692514016 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6446 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-03-29T14:22:11.812979Z","caller":"traceutil/trace.go:171","msg":"trace[2055985214] linearizableReadLoop","detail":"{readStateIndex:7994; appliedIndex:7993; }","duration":"1.888801769s","start":"2025-03-29T14:22:09.924148Z","end":"2025-03-29T14:22:11.812950Z","steps":["trace[2055985214] 'read index received'  (duration: 518.661142ms)","trace[2055985214] 'applied index is now lower than readState.Index'  (duration: 1.370137987s)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:11.814366Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.879919769s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:11.814436Z","caller":"traceutil/trace.go:171","msg":"trace[53455438] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6449; }","duration":"1.880040412s","start":"2025-03-29T14:22:09.934380Z","end":"2025-03-29T14:22:11.814421Z","steps":["trace[53455438] 'agreement among raft nodes before linearized reading'  (duration: 1.879917878s)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:11.814481Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:09.934356Z","time spent":"1.880113793s","remote":"127.0.0.1:57178","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-03-29T14:22:11.815209Z","caller":"traceutil/trace.go:171","msg":"trace[69734527] transaction","detail":"{read_only:false; response_revision:6449; number_of_response:1; }","duration":"1.88501723s","start":"2025-03-29T14:22:09.930170Z","end":"2025-03-29T14:22:11.815187Z","steps":["trace[69734527] 'process raft request'  (duration: 1.004139845s)","trace[69734527] 'compare'  (duration: 876.586072ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:11.815484Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:09.930145Z","time spent":"1.885131137s","remote":"127.0.0.1:57336","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6446 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-03-29T14:22:11.816005Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.891843051s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:11.816096Z","caller":"traceutil/trace.go:171","msg":"trace[616249258] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6449; }","duration":"1.891940242s","start":"2025-03-29T14:22:09.924140Z","end":"2025-03-29T14:22:11.816080Z","steps":["trace[616249258] 'agreement among raft nodes before linearized reading'  (duration: 1.891816902s)"],"step_count":1}
{"level":"info","ts":"2025-03-29T14:22:18.023762Z","caller":"traceutil/trace.go:171","msg":"trace[2070351142] transaction","detail":"{read_only:false; response_revision:6456; number_of_response:1; }","duration":"132.801043ms","start":"2025-03-29T14:22:17.890926Z","end":"2025-03-29T14:22:18.023727Z","steps":["trace[2070351142] 'process raft request'  (duration: 132.452704ms)"],"step_count":1}
{"level":"info","ts":"2025-03-29T14:22:20.426204Z","caller":"traceutil/trace.go:171","msg":"trace[1214733037] linearizableReadLoop","detail":"{readStateIndex:8005; appliedIndex:8004; }","duration":"384.249371ms","start":"2025-03-29T14:22:20.041933Z","end":"2025-03-29T14:22:20.426183Z","steps":["trace[1214733037] 'read index received'  (duration: 383.948496ms)","trace[1214733037] 'applied index is now lower than readState.Index'  (duration: 299.895µs)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:20.426719Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"384.762998ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-03-29T14:22:20.426947Z","caller":"traceutil/trace.go:171","msg":"trace[1120933904] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:6457; }","duration":"385.043334ms","start":"2025-03-29T14:22:20.041894Z","end":"2025-03-29T14:22:20.426937Z","steps":["trace[1120933904] 'agreement among raft nodes before linearized reading'  (duration: 384.690126ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:20.427183Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:20.041870Z","time spent":"385.296523ms","remote":"127.0.0.1:57582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":44,"response count":1,"response size":31,"request content":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true "}
{"level":"info","ts":"2025-03-29T14:22:20.426788Z","caller":"traceutil/trace.go:171","msg":"trace[1854618563] transaction","detail":"{read_only:false; response_revision:6457; number_of_response:1; }","duration":"388.615248ms","start":"2025-03-29T14:22:20.038157Z","end":"2025-03-29T14:22:20.426772Z","steps":["trace[1854618563] 'process raft request'  (duration: 387.864575ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:20.427692Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:20.038133Z","time spent":"389.501881ms","remote":"127.0.0.1:57336","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:6456 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-03-29T14:22:21.911196Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.25878875s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:21.912744Z","caller":"traceutil/trace.go:171","msg":"trace[682174884] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6457; }","duration":"1.260332233s","start":"2025-03-29T14:22:20.652318Z","end":"2025-03-29T14:22:21.912650Z","steps":["trace[682174884] 'range keys from in-memory index tree'  (duration: 1.258710946s)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:21.912963Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:20.652295Z","time spent":"1.26052507s","remote":"127.0.0.1:57178","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-03-29T14:22:21.911523Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.241695668s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:21.913514Z","caller":"traceutil/trace.go:171","msg":"trace[1245498064] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6457; }","duration":"1.243749835s","start":"2025-03-29T14:22:20.669733Z","end":"2025-03-29T14:22:21.913483Z","steps":["trace[1245498064] 'range keys from in-memory index tree'  (duration: 1.241608811s)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:21.912006Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"395.777957ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036224692514070 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:6448 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-03-29T14:22:21.916295Z","caller":"traceutil/trace.go:171","msg":"trace[1862943345] transaction","detail":"{read_only:false; response_revision:6458; number_of_response:1; }","duration":"1.175639346s","start":"2025-03-29T14:22:20.740149Z","end":"2025-03-29T14:22:21.915789Z","steps":["trace[1862943345] 'process raft request'  (duration: 775.98891ms)","trace[1862943345] 'compare'  (duration: 394.945749ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:21.916638Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:20.740130Z","time spent":"1.176339149s","remote":"127.0.0.1:57418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:6448 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-03-29T14:22:28.686505Z","caller":"traceutil/trace.go:171","msg":"trace[527903820] linearizableReadLoop","detail":"{readStateIndex:8014; appliedIndex:8013; }","duration":"202.259074ms","start":"2025-03-29T14:22:28.484223Z","end":"2025-03-29T14:22:28.686482Z","steps":["trace[527903820] 'read index received'  (duration: 155.435624ms)","trace[527903820] 'applied index is now lower than readState.Index'  (duration: 46.822179ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-29T14:22:28.686829Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"202.58679ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2025-03-29T14:22:28.686878Z","caller":"traceutil/trace.go:171","msg":"trace[753835181] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:6464; }","duration":"202.679161ms","start":"2025-03-29T14:22:28.484185Z","end":"2025-03-29T14:22:28.686865Z","steps":["trace[753835181] 'agreement among raft nodes before linearized reading'  (duration: 202.53692ms)"],"step_count":1}
{"level":"info","ts":"2025-03-29T14:22:39.026339Z","caller":"traceutil/trace.go:171","msg":"trace[1265300005] transaction","detail":"{read_only:false; response_revision:6474; number_of_response:1; }","duration":"204.648758ms","start":"2025-03-29T14:22:38.821676Z","end":"2025-03-29T14:22:39.026325Z","steps":["trace[1265300005] 'process raft request'  (duration: 204.547442ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:40.005486Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"351.684242ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:40.005562Z","caller":"traceutil/trace.go:171","msg":"trace[1286547984] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6474; }","duration":"351.798489ms","start":"2025-03-29T14:22:39.653748Z","end":"2025-03-29T14:22:40.005547Z","steps":["trace[1286547984] 'range keys from in-memory index tree'  (duration: 351.602118ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-29T14:22:40.005594Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-29T14:22:39.653727Z","time spent":"351.859757ms","remote":"127.0.0.1:57178","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-03-29T14:22:40.005855Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"335.241517ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-29T14:22:40.005905Z","caller":"traceutil/trace.go:171","msg":"trace[957158634] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6474; }","duration":"335.307751ms","start":"2025-03-29T14:22:39.670581Z","end":"2025-03-29T14:22:40.005889Z","steps":["trace[957158634] 'range keys from in-memory index tree'  (duration: 335.21433ms)"],"step_count":1}
{"level":"info","ts":"2025-03-29T14:23:02.210696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6214}
{"level":"info","ts":"2025-03-29T14:23:02.215629Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6214,"took":"4.433678ms","hash":2963063633,"current-db-size-bytes":2039808,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1499136,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-03-29T14:23:02.215776Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2963063633,"revision":6214,"compact-revision":5938}
{"level":"info","ts":"2025-03-29T14:28:02.219628Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6494}
{"level":"info","ts":"2025-03-29T14:28:02.223926Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6494,"took":"3.971714ms","hash":963858492,"current-db-size-bytes":2039808,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1196032,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-03-29T14:28:02.224002Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":963858492,"revision":6494,"compact-revision":6214}
{"level":"info","ts":"2025-03-29T14:30:06.578223Z","caller":"traceutil/trace.go:171","msg":"trace[1224206975] transaction","detail":"{read_only:false; response_revision:6842; number_of_response:1; }","duration":"120.402691ms","start":"2025-03-29T14:30:06.457805Z","end":"2025-03-29T14:30:06.578208Z","steps":["trace[1224206975] 'process raft request'  (duration: 120.302901ms)"],"step_count":1}


==> kernel <==
 14:32:20 up  3:29,  0 users,  load average: 0.87, 1.06, 1.23
Linux minikube 5.15.0-97-generic #107~20.04.1-Ubuntu SMP Fri Feb 9 14:20:11 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [5b6d85df3b69] <==
I0329 12:17:55.165350       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0329 12:17:55.165958       1 controller.go:142] Starting OpenAPI controller
I0329 12:17:55.165993       1 controller.go:90] Starting OpenAPI V3 controller
I0329 12:17:55.166110       1 naming_controller.go:294] Starting NamingConditionController
I0329 12:17:55.166197       1 establishing_controller.go:81] Starting EstablishingController
I0329 12:17:55.166221       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0329 12:17:55.166260       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0329 12:17:55.166456       1 crd_finalizer.go:269] Starting CRDFinalizer
I0329 12:17:55.258478       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0329 12:17:55.258497       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0329 12:17:55.261426       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0329 12:17:55.261610       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0329 12:17:55.445598       1 shared_informer.go:320] Caches are synced for node_authorizer
I0329 12:17:55.456014       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0329 12:17:55.456044       1 policy_source.go:240] refreshing policies
I0329 12:17:55.459817       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0329 12:17:55.459861       1 aggregator.go:171] initial CRD sync complete...
I0329 12:17:55.459868       1 autoregister_controller.go:144] Starting autoregister controller
I0329 12:17:55.459874       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0329 12:17:55.459879       1 cache.go:39] Caches are synced for autoregister controller
I0329 12:17:55.461711       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0329 12:17:55.463229       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0329 12:17:55.463382       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0329 12:17:55.463393       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0329 12:17:55.464501       1 cache.go:39] Caches are synced for LocalAvailability controller
I0329 12:17:55.464577       1 shared_informer.go:320] Caches are synced for configmaps
I0329 12:17:55.464724       1 controller.go:615] quota admission added evaluator for: namespaces
I0329 12:17:55.465090       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0329 12:17:55.465605       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
E0329 12:17:55.553108       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0329 12:17:55.556632       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0329 12:17:55.767402       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0329 12:17:56.341718       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0329 12:17:56.368624       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0329 12:17:56.368647       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0329 12:17:57.653180       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0329 12:17:57.737626       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0329 12:17:57.855349       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0329 12:17:57.865216       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0329 12:17:57.867240       1 controller.go:615] quota admission added evaluator for: endpoints
I0329 12:17:57.876488       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0329 12:17:58.388808       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0329 12:17:58.748636       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0329 12:17:58.775709       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0329 12:17:58.786503       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0329 12:18:04.358627       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0329 12:18:04.371357       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
E0329 12:23:25.645194       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0329 12:23:26.401739       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0329 12:23:26.403393       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0329 12:23:26.738939       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0329 12:23:27.492031       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.942112703s" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0329 12:23:32.159400       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0329 12:23:32.165177       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0329 12:23:32.185888       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0329 12:23:32.186089       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 5.781583ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0329 12:23:32.187383       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="28.173591ms" method="POST" path="/api/v1/namespaces/default/events" result=null
I0329 12:26:05.801155       1 alloc.go:330] "allocated clusterIPs" service="default/fastapi-service" clusterIPs={"IPv4":"10.109.184.213"}
I0329 12:29:12.953038       1 alloc.go:330] "allocated clusterIPs" service="default/redis-service" clusterIPs={"IPv4":"10.103.116.111"}
I0329 14:16:40.500268       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress-service" clusterIPs={"IPv4":"10.108.219.57"}


==> kube-controller-manager [8bf7fc65f3ed] <==
I0329 12:26:05.757834       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="119.564µs"
I0329 12:26:10.452192       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="62.9µs"
I0329 12:26:11.572314       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="54.282849ms"
I0329 12:26:11.573229       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="37.298µs"
I0329 12:26:12.775192       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="31.018µs"
I0329 12:26:13.829934       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="40.513417ms"
I0329 12:26:13.829997       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="32.461µs"
I0329 12:26:32.310832       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:28:36.862553       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="83.846µs"
I0329 12:28:36.898525       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/fastapi-deployment-cf4dc69bc" duration="72.975µs"
I0329 12:29:12.822798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="48.535229ms"
I0329 12:29:12.890727       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="67.795222ms"
I0329 12:29:12.913737       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="22.884752ms"
I0329 12:29:12.913873       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="45.845µs"
I0329 12:29:22.319675       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="13.076462ms"
I0329 12:29:22.320971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/redis-deployment-748ffbc5f5" duration="18µs"
I0329 12:29:35.572890       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:35:13.051184       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:40:19.557803       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:45:25.764300       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:50:31.734236       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 12:55:38.198333       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:00:43.492790       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:05:50.355021       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:10:57.390966       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:16:03.318099       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:21:08.452735       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:26:14.055240       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:31:20.043829       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:36:24.129290       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:41:29.441946       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:46:38.793502       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:51:44.892547       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 13:56:50.565561       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:01:57.367095       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:07:03.361965       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:12:09.064562       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:16:40.793187       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="153.115673ms"
I0329 14:16:40.814916       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="21.462941ms"
I0329 14:16:40.815098       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="53.391µs"
I0329 14:16:40.861506       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="52.284µs"
I0329 14:16:40.909001       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="46.966µs"
I0329 14:16:59.144597       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="144.168µs"
I0329 14:17:13.952039       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="81.515µs"
I0329 14:17:15.337173       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:17:42.953983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="77.13µs"
I0329 14:17:55.957798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="44.779µs"
I0329 14:18:01.221780       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-qfslg" approvedExpiration="1h0m0s"
I0329 14:18:26.961273       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="75.099µs"
I0329 14:18:41.947058       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="64.821µs"
I0329 14:19:23.951261       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="69.43µs"
I0329 14:19:38.950924       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="61.214µs"
I0329 14:21:12.951007       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="197.081µs"
I0329 14:21:23.960035       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="107.613µs"
I0329 14:22:22.060047       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:24:20.948059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="81.039µs"
I0329 14:24:33.947396       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="69.842µs"
I0329 14:27:27.557458       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0329 14:29:46.964706       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="61.411µs"
I0329 14:30:00.953644       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/wordpress-deployment-594ffdd65f" duration="79.529µs"


==> kube-proxy [2117abf446cf] <==
I0329 12:18:08.394345       1 server_linux.go:66] "Using iptables proxy"
I0329 12:18:08.653078       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0329 12:18:08.653222       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0329 12:18:08.703786       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0329 12:18:08.703903       1 server_linux.go:170] "Using iptables Proxier"
I0329 12:18:08.714826       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0329 12:18:08.730820       1 server.go:497] "Version info" version="v1.32.0"
I0329 12:18:08.730861       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 12:18:09.286974       1 config.go:105] "Starting endpoint slice config controller"
I0329 12:18:09.296488       1 config.go:329] "Starting node config controller"
I0329 12:18:09.296514       1 shared_informer.go:313] Waiting for caches to sync for node config
I0329 12:18:09.309542       1 config.go:199] "Starting service config controller"
I0329 12:18:09.309574       1 shared_informer.go:313] Waiting for caches to sync for service config
I0329 12:18:09.309616       1 shared_informer.go:320] Caches are synced for service config
I0329 12:18:09.309622       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0329 12:18:09.309625       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0329 12:18:09.401517       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [aed2f8903583] <==
E0329 12:17:55.428161       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0329 12:17:55.428418       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0329 12:17:55.426485       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0329 12:17:55.426503       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0329 12:17:55.426517       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0329 12:17:55.426542       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0329 12:17:55.426565       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0329 12:17:55.426665       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0329 12:17:55.426722       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0329 12:17:55.426758       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0329 12:17:55.427083       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0329 12:17:55.427185       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0329 12:17:55.427361       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0329 12:17:55.427412       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:55.429513       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429220       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429230       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429265       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429933       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429062       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429970       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429085       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429981       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429095       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429196       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.430016       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0329 12:17:55.429210       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.237451       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0329 12:17:56.237791       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.317309       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:56.317359       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.320371       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:56.320511       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.481416       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0329 12:17:56.481458       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.503845       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:56.503884       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.523277       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0329 12:17:56.523352       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.627900       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0329 12:17:56.627996       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.631569       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0329 12:17:56.631608       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.678790       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0329 12:17:56.678888       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.703632       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0329 12:17:56.703886       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.784871       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:56.785100       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.832975       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0329 12:17:56.833048       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.869526       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0329 12:17:56.869596       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0329 12:17:56.919331       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0329 12:17:56.923104       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0329 12:17:56.999858       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0329 12:17:57.000125       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0329 12:17:57.030029       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0329 12:17:57.030334       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
I0329 12:17:58.698485       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 29 14:22:15 minikube kubelet[2389]: E0329 14:22:15.931275    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:22:29 minikube kubelet[2389]: E0329 14:22:29.929646    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:22:45 minikube kubelet[2389]: E0329 14:22:45.004401    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:22:56 minikube kubelet[2389]: E0329 14:22:56.937114    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:23:09 minikube kubelet[2389]: E0329 14:23:09.929753    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:23:23 minikube kubelet[2389]: E0329 14:23:23.927869    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:23:38 minikube kubelet[2389]: E0329 14:23:38.930267    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:24:09 minikube kubelet[2389]: E0329 14:24:09.964540    2389 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="wordpress:5.7"
Mar 29 14:24:09 minikube kubelet[2389]: E0329 14:24:09.964621    2389 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="wordpress:5.7"
Mar 29 14:24:09 minikube kubelet[2389]: E0329 14:24:09.964792    2389 kuberuntime_manager.go:1341] "Unhandled Error" err=<
Mar 29 14:24:09 minikube kubelet[2389]:         container &Container{Name:wordpress,Image:wordpress:5.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:redis-service:6379,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_USER,Value:,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:nil,},EnvVar{Name:WORDPRESS_CONFIG_EXTRA,Value:define('WP_ALLOW_MULTISITE', true);
Mar 29 14:24:09 minikube kubelet[2389]:         define('DISABLE_WP_CRON', true);
Mar 29 14:24:09 minikube kubelet[2389]:         define('WP_AUTO_UPDATE_CORE', false);
Mar 29 14:24:09 minikube kubelet[2389]:         ,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DEBUG,Value:1,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-data,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-x99wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod wordpress-deployment-594ffdd65f-sxrjh_default(9e883f17-6005-42fb-aa61-d8f1ca51e62f): ErrImagePull: Error response from daemon: Get "https://registry-1.docker.io/v2/": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving
Mar 29 14:24:09 minikube kubelet[2389]:  > logger="UnhandledError"
Mar 29 14:24:09 minikube kubelet[2389]: E0329 14:24:09.966643    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:24:20 minikube kubelet[2389]: E0329 14:24:20.931004    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:24:33 minikube kubelet[2389]: E0329 14:24:33.928925    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:24:46 minikube kubelet[2389]: E0329 14:24:46.932109    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:24:57 minikube kubelet[2389]: E0329 14:24:57.930664    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:25:11 minikube kubelet[2389]: E0329 14:25:11.929476    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:25:22 minikube kubelet[2389]: E0329 14:25:22.928699    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:25:33 minikube kubelet[2389]: E0329 14:25:33.930894    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:25:47 minikube kubelet[2389]: E0329 14:25:47.928537    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:26:02 minikube kubelet[2389]: E0329 14:26:02.929403    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:26:17 minikube kubelet[2389]: E0329 14:26:17.928017    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:26:29 minikube kubelet[2389]: E0329 14:26:29.939610    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:26:44 minikube kubelet[2389]: E0329 14:26:44.930331    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:26:58 minikube kubelet[2389]: E0329 14:26:58.936265    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:27:12 minikube kubelet[2389]: E0329 14:27:12.932420    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:27:26 minikube kubelet[2389]: E0329 14:27:26.928235    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:27:37 minikube kubelet[2389]: E0329 14:27:37.930768    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:27:52 minikube kubelet[2389]: E0329 14:27:52.930969    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:28:04 minikube kubelet[2389]: E0329 14:28:04.927933    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:28:15 minikube kubelet[2389]: E0329 14:28:15.933207    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:28:26 minikube kubelet[2389]: E0329 14:28:26.929455    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:28:37 minikube kubelet[2389]: E0329 14:28:37.931151    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:28:51 minikube kubelet[2389]: E0329 14:28:51.931032    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:29:03 minikube kubelet[2389]: E0329 14:29:03.930786    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:29:34 minikube kubelet[2389]: E0329 14:29:34.961579    2389 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="wordpress:5.7"
Mar 29 14:29:34 minikube kubelet[2389]: E0329 14:29:34.961739    2389 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving" image="wordpress:5.7"
Mar 29 14:29:34 minikube kubelet[2389]: E0329 14:29:34.962083    2389 kuberuntime_manager.go:1341] "Unhandled Error" err=<
Mar 29 14:29:34 minikube kubelet[2389]:         container &Container{Name:wordpress,Image:wordpress:5.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:redis-service:6379,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_USER,Value:,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:nil,},EnvVar{Name:WORDPRESS_CONFIG_EXTRA,Value:define('WP_ALLOW_MULTISITE', true);
Mar 29 14:29:34 minikube kubelet[2389]:         define('DISABLE_WP_CRON', true);
Mar 29 14:29:34 minikube kubelet[2389]:         define('WP_AUTO_UPDATE_CORE', false);
Mar 29 14:29:34 minikube kubelet[2389]:         ,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DEBUG,Value:1,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-data,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-x99wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod wordpress-deployment-594ffdd65f-sxrjh_default(9e883f17-6005-42fb-aa61-d8f1ca51e62f): ErrImagePull: Error response from daemon: Get "https://registry-1.docker.io/v2/": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving
Mar 29 14:29:34 minikube kubelet[2389]:  > logger="UnhandledError"
Mar 29 14:29:34 minikube kubelet[2389]: E0329 14:29:34.964086    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:29:46 minikube kubelet[2389]: E0329 14:29:46.942063    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:30:00 minikube kubelet[2389]: E0329 14:30:00.929697    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:30:13 minikube kubelet[2389]: E0329 14:30:13.931439    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:30:26 minikube kubelet[2389]: E0329 14:30:26.930404    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:30:37 minikube kubelet[2389]: E0329 14:30:37.931342    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:30:48 minikube kubelet[2389]: E0329 14:30:48.935715    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:31:02 minikube kubelet[2389]: E0329 14:31:02.930889    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:31:13 minikube kubelet[2389]: E0329 14:31:13.930994    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:31:24 minikube kubelet[2389]: E0329 14:31:24.931208    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:31:39 minikube kubelet[2389]: E0329 14:31:39.928124    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:31:52 minikube kubelet[2389]: E0329 14:31:52.931577    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"
Mar 29 14:32:05 minikube kubelet[2389]: E0329 14:32:05.930387    2389 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"wordpress:5.7\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp: lookup registry-1.docker.io on 192.168.49.1:53: server misbehaving\"" pod="default/wordpress-deployment-594ffdd65f-sxrjh" podUID="9e883f17-6005-42fb-aa61-d8f1ca51e62f"


==> storage-provisioner [39af22247098] <==
I0329 12:23:53.280625       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0329 12:23:53.300768       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0329 12:23:53.300847       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0329 12:24:10.727962       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0329 12:24:10.728298       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8a8a5f1c-6f86-4050-b847-92c88ecb30e7!
I0329 12:24:10.732015       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a608fd46-f5bb-4e08-b8ee-0ee2e852e9f2", APIVersion:"v1", ResourceVersion:"680", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8a8a5f1c-6f86-4050-b847-92c88ecb30e7 became leader
I0329 12:24:10.829340       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8a8a5f1c-6f86-4050-b847-92c88ecb30e7!


==> storage-provisioner [5075443048c7] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000519580)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000519580, 0x18b3d60, 0xc00051cc30, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000519580, 0x3b9aca00, 0x0, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000519580, 0x3b9aca00, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 86 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc000123950, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000123940)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0004422a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc000128f00, 0x18e5530, 0xc0000b9800, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0005195a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0005195a0, 0x18b3d60, 0xc00051cc60, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0005195a0, 0x3b9aca00, 0x0, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0005195a0, 0x3b9aca00, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 87 [sync.Cond.Wait, 4 minutes]:
sync.runtime_notifyListWait(0xc0001239d0, 0x0)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0001239c0)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000442420, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc000128f00, 0x18e5530, 0xc0000b9800, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0005195c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0005195c0, 0x18b3d60, 0xc00051cb10, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0005195c0, 0x3b9aca00, 0x0, 0x1, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0005195c0, 0x3b9aca00, 0xc000130a20)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

